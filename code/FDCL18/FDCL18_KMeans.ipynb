{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import normalize\n",
    "from torch.amp import GradScaler, autocast\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import platform\n",
    "import sys\n",
    "import sklearn\n",
    "import transformers\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "seed = 677\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "tokenizer_model = \"roberta-base\"\n",
    "training_group = \"whole\"  # AA, White, whole\n",
    "percent_per_cluster = 0.25\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "code = \"KMeans\"\n",
    "\n",
    "\n",
    "training_df = pd.read_csv(\"FDCL18_train.csv\", sep=\"\\t\")\n",
    "validation_df = pd.read_csv(\"FDCL18_validation.csv\", sep=\"\\t\")\n",
    "test_df = pd.read_csv(\"FDCL18_test.csv\", sep=\"\\t\")\n",
    "print(training_df.columns)\n",
    "\n",
    "\n",
    "X_train = training_df.drop(columns='label')\n",
    "X_valid = validation_df.drop(columns='label')\n",
    "X_test = test_df.drop(columns='label')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train = pd.Series(label_encoder.fit_transform(training_df['label']), index=training_df.index)\n",
    "y_valid = pd.Series(label_encoder.transform(validation_df['label']), index=validation_df.index)\n",
    "y_test = pd.Series(label_encoder.transform(test_df['label']), index=test_df.index)\n",
    "\n",
    "print(\"\\nLabel Encoding Map (index → label):\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "\n",
    "X_train_AA = X_train[X_train['dialect'] == 'AA'].copy()\n",
    "y_train_AA = y_train[X_train['dialect'] == 'AA']\n",
    "X_valid_AA = X_valid[X_valid['dialect'] == 'AA'].copy()\n",
    "y_valid_AA = y_valid[X_valid['dialect'] == 'AA']\n",
    "X_test_White = X_test[X_test['dialect'] == 'White'].copy()\n",
    "y_test_White = y_test[X_test['dialect'] == 'White']\n",
    "X_test_AA = X_test[X_test['dialect'] == 'AA'].copy()\n",
    "y_test_AA = y_test[X_test['dialect'] == 'AA']\n",
    "\n",
    "\n",
    "assert X_train_AA['dialect'].nunique() == 1 and X_train_AA['dialect'].iloc[0] == 'AA'\n",
    "assert X_valid_AA['dialect'].nunique() == 1 and X_valid_AA['dialect'].iloc[0] == 'AA'\n",
    "assert X_test_AA['dialect'].nunique() == 1 and X_test_AA['dialect'].iloc[0] == 'AA'\n",
    "assert X_test_White['dialect'].nunique() == 1 and X_test_White['dialect'].iloc[0] == 'White'\n",
    "\n",
    "\n",
    "if training_group == \"AA\":\n",
    "    X_train_group, y_train_group = X_train_AA['tweet'], y_train_AA\n",
    "    X_valid_group, y_valid_group = X_valid_AA['tweet'], y_valid_AA\n",
    "elif training_group == \"White\":\n",
    "    X_train_White = X_train[X_train['dialect'] == 'White'].copy()\n",
    "    y_train_White = y_train[X_train['dialect'] == 'White']\n",
    "    X_valid_White = X_valid[X_valid['dialect'] == 'White'].copy()\n",
    "    y_valid_White = y_valid[X_valid['dialect'] == 'White']\n",
    "    X_train_group, y_train_group = X_train_White['tweet'], y_train_White\n",
    "    X_valid_group, y_valid_group = X_valid_White['tweet'], y_valid_White\n",
    "\n",
    "    assert X_train_White['dialect'].nunique() == 1 and X_train_White['dialect'].iloc[0] == 'White'\n",
    "    assert X_valid_White['dialect'].nunique() == 1 and X_valid_White['dialect'].iloc[0] == 'White'\n",
    "\n",
    "elif training_group == \"whole\":\n",
    "    assert set(X_train['dialect'].unique()) == {'White', 'AA'}\n",
    "    assert set(X_valid['dialect'].unique()) == {'White', 'AA'}\n",
    "    X_train_group, y_train_group = X_train['tweet'], y_train\n",
    "    X_valid_group, y_valid_group = X_valid['tweet'], y_valid\n",
    "\n",
    "print(f\"[INFO] Training group: {training_group} — Number of available training examples: {len(X_train_group)}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "\n",
    "class FDCL_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(texts, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing Embeddings\"):\n",
    "        batch = texts.iloc[i:i + batch_size]\n",
    "        encodings = tokenizer(list(batch), padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, return_dict=True)\n",
    "            cls_embeddings = outputs.hidden_states[-1][:, 0, :]\n",
    "            cls_embeddings = normalize(cls_embeddings, dim=1)\n",
    "            embeddings.append(cls_embeddings.cpu())\n",
    "    return torch.cat(embeddings, dim=0).to(device)\n",
    "\n",
    "\n",
    "def select_representative_coreset(embeddings, kmeans_model, cluster_labels, percent_per_cluster=0.25, log_path = f\"KMeans_selection_{tokenizer_model}_{training_group}_{seed}.txt\"):\n",
    "    embeddings_np = embeddings.cpu().numpy()\n",
    "    selected_indices = []\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        for cluster_id in tqdm(range(len(kmeans_model.cluster_centers_)), desc=\"Selecting representative points\"):\n",
    "            cluster_idx = np.where(cluster_labels == cluster_id)[0]\n",
    "            cluster_embeds = embeddings_np[cluster_idx]\n",
    "            cluster_center = kmeans_model.cluster_centers_[cluster_id]\n",
    "            distances = np.linalg.norm(cluster_embeds - cluster_center, axis=1)\n",
    "            n_select = max(1, int(percent_per_cluster * len(cluster_idx)))\n",
    "            closest_indices = np.argsort(distances)[:n_select]\n",
    "\n",
    "            f.write(f\"\\nCluster {cluster_id}:\\n\")\n",
    "            f.write(f\"  Total in cluster: {len(cluster_idx)}\\n\")\n",
    "            f.write(f\"  Selecting {n_select} closest points\\n\")\n",
    "            f.write(f\"  Closest distances: {np.sort(distances)[:n_select]}\\n\")\n",
    "            f.write(f\"  Farthest distance in selection: {np.sort(distances)[:n_select][-1]:.4f}\\n\")\n",
    "            f.write(f\"  Max possible distance in cluster: {np.max(distances):.4f}\\n\")\n",
    "\n",
    "            selected_indices.extend(cluster_idx[closest_indices])\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "# Visualization\n",
    "def project_embeddings_2d(embeddings):\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "    return tsne.fit_transform(embeddings.cpu().numpy())\n",
    "\n",
    "\n",
    "def plot_kmeans_clusters_2(embeddings_2d, selected_indices, save_path):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "\n",
    "    plt.scatter(\n",
    "        embeddings_2d[:, 0],\n",
    "        embeddings_2d[:, 1],\n",
    "        c='gray',\n",
    "        s=10,\n",
    "        alpha=0.5,\n",
    "        label='Unselected',\n",
    "        edgecolors='none'\n",
    "    )\n",
    "    plt.scatter(\n",
    "        embeddings_2d[selected_indices, 0],\n",
    "        embeddings_2d[selected_indices, 1],\n",
    "        c='red',\n",
    "        s=10,\n",
    "        alpha=0.7,\n",
    "        label='Selected',\n",
    "        edgecolors='none'\n",
    "    )\n",
    "    plt.title(\"t-SNE Visualization of Embedding Space\\n(Selected vs Unselected Samples)\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "embedding_model = AutoModelForSequenceClassification.from_pretrained(tokenizer_model, num_labels=num_labels).to(device)\n",
    "\n",
    "for param in embedding_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "embeddings = compute_embeddings(X_train_group, embedding_model, tokenizer, device)\n",
    "candidate_ks = [2, 3, 4, 5, 10, 15] # check for the best one\n",
    "scores = {}\n",
    "models = {}\n",
    "for k in tqdm(candidate_ks, desc=\"Evaluating cluster sizes\"):\n",
    "    km = KMeans(n_clusters=k, random_state=seed, n_init='auto')\n",
    "    labels = km.fit_predict(embeddings.cpu().numpy())\n",
    "    score = silhouette_score(embeddings.cpu().numpy(), labels)\n",
    "    scores[k] = score\n",
    "    models[k] = (km, labels)\n",
    "best_k = max(scores, key=scores.get)\n",
    "kmeans_model, cluster_labels = models[best_k]\n",
    "print(f\"Best K = {best_k} with Silhouette Score = {scores[best_k]:.4f}\")\n",
    "\n",
    "selected_indices = select_representative_coreset(embeddings, kmeans_model, cluster_labels, percent_per_cluster)\n",
    "\n",
    "index_df = pd.DataFrame({\"selected_indices\": sorted(X_train_group.iloc[selected_indices].index)})\n",
    "index_df.to_csv(f\"KMeans_{training_group}_{tokenizer_model}_{seed}.csv\", index=False)\n",
    "\n",
    "embeddings_2d = project_embeddings_2d(embeddings)\n",
    "\n",
    "plot_kmeans_clusters_2(embeddings_2d, selected_indices, save_path=f\"KMeans_k{best_k}_{training_group}_{tokenizer_model}_{seed}.png\")\n",
    "\n",
    "X_train_core = X_train_group.iloc[selected_indices]\n",
    "print(f\"[INFO] Number of selected examples: {len(X_train_core)}\")\n",
    "y_train_core = y_train_group.iloc[selected_indices]\n",
    "train_dataset = FDCL_dataset(tokenize_function(X_train_core), y_train_core)\n",
    "valid_dataset = FDCL_dataset(tokenize_function(X_valid_group), y_valid_group)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(tokenizer_model, num_labels=num_labels).to(device)\n",
    "\n",
    "# lr\n",
    "if tokenizer_model == \"bert-base-uncased\":\n",
    "    lr = 2e-5\n",
    "elif tokenizer_model == \"roberta-base\":\n",
    "    lr = 2e-5\n",
    "elif tokenizer_model == \"distilroberta-base\":\n",
    "    lr = 5e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids, attention_mask=mask).logits\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    decoded_preds = label_encoder.inverse_transform(preds)\n",
    "    decoded_labels = label_encoder.inverse_transform(true_labels)\n",
    "    print(classification_report(decoded_labels, decoded_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_sets = {\n",
    "    \"whole\": (X_test['tweet'], y_test),\n",
    "    \"White\": (X_test_White['tweet'], y_test_White),\n",
    "    \"AA\": (X_test_AA['tweet'], y_test_AA)\n",
    "}\n",
    "print(f\"\\n*** Used code: {code}. Training group: {training_group}. Model: {tokenizer_model}. Seed: {seed} ***\")\n",
    "for name, (Xg, yg) in test_sets.items():\n",
    "    print(f\"\\n--- Testing on {name.upper()} ---\")\n",
    "    test_data = FDCL_dataset(tokenize_function(Xg), yg)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=32)\n",
    "    model.eval()\n",
    "    preds, labels_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Testing {name}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids, attention_mask=mask).logits\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels_all.extend(labels.cpu().numpy())\n",
    "    decoded_preds = label_encoder.inverse_transform(preds)\n",
    "    decoded_labels = label_encoder.inverse_transform(labels_all)\n",
    "    print(classification_report(decoded_labels, decoded_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "    # SAVE TO EXCEL\n",
    "    report_dict = classification_report(\n",
    "        decoded_labels,\n",
    "        decoded_preds,\n",
    "        target_names=label_encoder.classes_,\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "    if \"accuracy\" in report_df.index:\n",
    "        report_df.loc[\"accuracy\", [\"precision\", \"recall\"]] = [float(\"nan\"), float(\"nan\")]\n",
    "        report_df.loc[\"accuracy\", \"support\"] = len(decoded_labels)\n",
    "\n",
    "    report_df[\"support\"] = pd.to_numeric(report_df[\"support\"], errors=\"coerce\").round()\n",
    "\n",
    "    report_df.loc[report_df.index != \"accuracy\", \"support\"] = (\n",
    "        report_df.loc[report_df.index != \"accuracy\", \"support\"].astype(\"Int64\")\n",
    "    )\n",
    "\n",
    "    for col in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "        if col in report_df.columns:\n",
    "            report_df[col] = pd.to_numeric(report_df[col], errors=\"coerce\").round(2)\n",
    "\n",
    "    report_df = report_df.astype(str)\n",
    "    report_df.to_excel(f\"classification_report_{code}_{tokenizer_model}_{seed}_{training_group}_{name}.xlsx\")\n",
    "    print(\"#####################################################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
