{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea95065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import normalize\n",
    "from torch.amp import GradScaler, autocast\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import platform\n",
    "import sys\n",
    "import sklearn\n",
    "import transformers\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 677\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "tokenizer_model = \"roberta-base\"\n",
    "training_group = \"whole\"  # females, males, whole\n",
    "coreset_fraction = 0.25\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "code = \"KCG\"\n",
    "\n",
    "\n",
    "training_df = pd.read_csv(\"PAN16_training_df.csv\")\n",
    "validation_df = pd.read_csv(\"PAN16_validation_df.csv\")\n",
    "test_df = pd.read_csv(\"PAN16_test_df.csv\")\n",
    "\n",
    "\n",
    "X_train = training_df.drop(columns='task_label')\n",
    "y_train = training_df['task_label']\n",
    "X_valid = validation_df.drop(columns='task_label')\n",
    "y_valid = validation_df['task_label']\n",
    "X_test = test_df.drop(columns='task_label')\n",
    "y_test = test_df['task_label']\n",
    "\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_valid) == len(y_valid)\n",
    "assert len(X_test) == len(y_test)\n",
    "print(\"Split integrity verified\")\n",
    "\n",
    "\n",
    "X_train_males = X_train[X_train['gender'] == 'male'].copy()\n",
    "y_train_males = y_train[X_train['gender'] == 'male']\n",
    "X_train_females = X_train[X_train['gender'] == 'female'].copy()\n",
    "y_train_females = y_train[X_train['gender'] == 'female']\n",
    "\n",
    "X_valid_males = X_valid[X_valid['gender'] == 'male'].copy()\n",
    "y_valid_males = y_valid[X_valid['gender'] == 'male']\n",
    "X_valid_females = X_valid[X_valid['gender'] == 'female'].copy()\n",
    "y_valid_females = y_valid[X_valid['gender'] == 'female']\n",
    "\n",
    "X_test_males = X_test[X_test['gender'] == 'male'].copy()\n",
    "y_test_males = y_test[X_test['gender'] == 'male']\n",
    "X_test_females = X_test[X_test['gender'] == 'female'].copy()\n",
    "y_test_females = y_test[X_test['gender'] == 'female']\n",
    "\n",
    "assert X_train_males['gender'].nunique() == 1\n",
    "assert X_train_females['gender'].nunique() == 1\n",
    "assert X_valid_males['gender'].nunique() == 1\n",
    "assert X_valid_females['gender'].nunique() == 1\n",
    "assert X_test_males['gender'].nunique() == 1\n",
    "assert X_test_females['gender'].nunique() == 1\n",
    "print(\"Gender splits confirmed\")\n",
    "\n",
    "\n",
    "if training_group == \"females\":\n",
    "    X_train_group, y_train_group = X_train_females['text'], y_train_females\n",
    "    X_valid_group, y_valid_group = X_valid_females['text'], y_valid_females\n",
    "elif training_group == \"males\":\n",
    "    X_train_group, y_train_group = X_train_males['text'], y_train_males\n",
    "    X_valid_group, y_valid_group = X_valid_males['text'], y_valid_males\n",
    "else:\n",
    "    X_train_group, y_train_group = X_train['text'], y_train\n",
    "    X_valid_group, y_valid_group = X_valid['text'], y_valid\n",
    "\n",
    "print(f\"[INFO] Training group: {training_group} â€” Number of available training examples: {len(X_train_group)}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "\n",
    "class PAN16DATASET(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(texts, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing Embeddings\"):\n",
    "        batch = texts.iloc[i:i + batch_size]\n",
    "        encodings = tokenizer(list(batch), padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, return_dict=True)\n",
    "            cls_embeddings = outputs.hidden_states[-1][:, 0, :]\n",
    "            cls_embeddings = normalize(cls_embeddings, dim=1)\n",
    "            embeddings.append(cls_embeddings.cpu())\n",
    "    return torch.cat(embeddings, dim=0).to(device)\n",
    "\n",
    "\n",
    "def k_center_greedy(embeddings, k):\n",
    "    selected = [torch.randint(len(embeddings), (1,), device=embeddings.device).item()]\n",
    "    distances = torch.cdist(embeddings, embeddings[selected]).min(dim=1).values\n",
    "\n",
    "    for _ in trange(1, k, desc=\"K-Greedy Selection\"):\n",
    "        idx = torch.argmax(distances).item()\n",
    "        selected.append(idx)\n",
    "        new_dist = torch.cdist(embeddings, embeddings[[idx]]).squeeze()\n",
    "        distances = torch.minimum(distances, new_dist)\n",
    "    return selected\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model = AutoModelForSequenceClassification.from_pretrained(tokenizer_model, num_labels=2).to(device)\n",
    "for param in embedding_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "embeddings = compute_embeddings(X_train_group, embedding_model, tokenizer, device)\n",
    "k = int(coreset_fraction * len(X_train_group))\n",
    "print(f\"INFO: Number of selected training examples: {k}\")\n",
    "selected_indices = k_center_greedy(embeddings, k)\n",
    "X_train_core = X_train_group.iloc[selected_indices]\n",
    "y_train_core = y_train_group.iloc[selected_indices]\n",
    "\n",
    "# Visualization\n",
    "embeddings_np = embeddings.cpu().numpy()\n",
    "selected_positions = np.array(selected_indices)\n",
    "assert selected_positions.max() < len(embeddings), \"Index out of bounds in selection\"\n",
    "selected_mask = np.zeros(len(embeddings), dtype=bool)\n",
    "selected_mask[selected_positions] = True\n",
    "print(f\"Selected points: {selected_mask.sum()} / {len(selected_mask)} total\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=seed, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_np)\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "plt.scatter(\n",
    "    embeddings_2d[~selected_mask, 0],\n",
    "    embeddings_2d[~selected_mask, 1],\n",
    "    c='gray', alpha=0.5, label='Unselected',\n",
    "    s=10, edgecolors='none'\n",
    ")\n",
    "plt.scatter(\n",
    "    embeddings_2d[selected_mask, 0],\n",
    "    embeddings_2d[selected_mask, 1],\n",
    "    c='red', alpha=0.7, label='Selected',\n",
    "    s=10, edgecolors='none'\n",
    ")\n",
    "plt.title(\"t-SNE Visualization of Embedding Space (K-Center Greedy)\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"DS_{training_group}_{tokenizer_model}_{seed}.png\", dpi=300) # png\n",
    "plt.savefig(f\"DS_{training_group}_{tokenizer_model}_{seed}.pdf\", bbox_inches='tight') # pdf\n",
    "\n",
    "\n",
    "index_file = f\"DS_indices_{training_group}_{tokenizer_model}_{seed}.csv\"\n",
    "index_df = pd.DataFrame({\"selected_indices\": sorted(X_train_core.index)})\n",
    "index_df.to_csv(index_file, index=False)\n",
    "print(f\"Saved coreset indices to: {index_file}\")\n",
    "\n",
    "\n",
    "train_dataset = PAN16DATASET(tokenize_function(X_train_core), y_train_core)\n",
    "valid_dataset = PAN16DATASET(tokenize_function(X_valid_group), y_valid_group)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(tokenizer_model, num_labels=2).to(device)\n",
    "if tokenizer_model == \"bert-base-uncased\":\n",
    "    lr = 2e-5\n",
    "elif tokenizer_model == \"roberta-base\":\n",
    "    lr = 2e-5\n",
    "elif tokenizer_model == \"distilroberta-base\":\n",
    "    lr = 5e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids, attention_mask=mask).logits\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    print(classification_report(true_labels, preds, target_names=[\"No Mention (0)\", \"Mention (1)\"]))\n",
    "\n",
    "\n",
    "test_sets = {\n",
    "    \"whole\": (X_test['text'], y_test),\n",
    "    \"males\": (X_test_males['text'], y_test_males),\n",
    "    \"females\": (X_test_females['text'], y_test_females)\n",
    "}\n",
    "\n",
    "print(f\"\\n*** Used code: {code}. Training group: {training_group}. Model: {tokenizer_model}. Seed: {seed} ***\")\n",
    "for name, (Xg, yg) in test_sets.items():\n",
    "    print(f\"\\n--- Testing on {name.upper()} ---\")\n",
    "    test_data = PAN16DATASET(tokenize_function(Xg), yg)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=32)\n",
    "    model.eval()\n",
    "    preds, labels_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Testing {name}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids, attention_mask=mask).logits\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels_all.extend(labels.cpu().numpy())\n",
    "    print(classification_report(labels_all, preds, target_names=[\"No Mention (0)\", \"Mention (1)\"]))\n",
    "    print(\"#####################################################\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
